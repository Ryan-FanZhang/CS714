{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52915c08-2155-4325-a96f-fa2ff9c84e22",
   "metadata": {},
   "source": [
    "# COMPSCI 714 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e4bd1-3d79-450b-bff4-a09eeb234e60",
   "metadata": {},
   "source": [
    "This assignment will evaluate the content covered in weeks 1 & 2. This assignment aims at evaluating your coding skills, aligned with what we covered in the lectutorials, but also your critical thinking and self-learning abilities with a few more advanced questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6a36f-fbe7-4917-a496-449c23c18c1f",
   "metadata": {},
   "source": [
    "**Name**: \n",
    "\n",
    "**UPI**: \n",
    "\n",
    "**Student ID**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bc6af-baf8-4514-b7e6-67a85e3283a1",
   "metadata": {},
   "source": [
    "## **Part 1: Data loading, exploration, cleaning and pre-processing** - 17 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fe41d-35a4-4d4f-b226-268720a7778c",
   "metadata": {},
   "source": [
    "Some of the libraries useful to this part are found in the following cell. You might need to import some more yourself for some of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f49140-6a7e-4726-a981-f63531df4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77454890-75c8-4af8-b7c8-ae263fb50c90",
   "metadata": {},
   "source": [
    "### **Task 1: Load the dataset** - 1 mark\n",
    "The dataset used in the assignment contains multiple attributes of houses and their target sale price stored in `SalePrice`. \\\n",
    "You can find information about each attribute in the \"Housing Price Data Dictionnary.txt\" file.\n",
    "\n",
    "**Task deliverable**:\n",
    "- Load the dataset contained in the file *house_prices.csv* as a Pandas DataFrame in a variable called `df_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0aa1bf-beab-4734-97e1-a85e3dd0a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4226ba-9348-4a71-a793-1109b8e544a6",
   "metadata": {},
   "source": [
    "### **Task 2: Extract basic dataset information** - 2 marks\n",
    "\n",
    "**Task deliverable**:\n",
    "\n",
    "Complete the `extract_basic_info()` function which takes a tabular dataset in the form of a Pandas Dataframe as parameter, and returns a tuple containing the following information:\n",
    "- The number of instances in the dataset.\n",
    "- The total number of attributes in the dataset.\n",
    "- The number of numerical attributes.\n",
    "- The number of categorical attributes.\n",
    "- The total memory size of the dataset.\n",
    "\n",
    "The order of the elements in the returned tuple should follow the same order as follow: (number of instances, totoal number of attributes, number of numerical attributes, number of categorical attributes, total memory size).\n",
    "\n",
    "**Remarks**: \n",
    "- For this task, we define a numerical attribute as an attribute having numeric values. These values could represent a continuous or categorical attribute. Contrastively, we define a symbolic attribute as an attribute having non-numeric values.\n",
    "- During grading, your function will be tested with the dataset previously loaded, but also with other hidden datasets. Make sure to test it to make sure it works fine on a few other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61cf43-2be8-4f6f-ac44-b99dcfb1e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_info(df_dataset):\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05775003-eaa9-4235-a1b8-422f6cad3cb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "### **Task 3: Basic exploration** - 2 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0cc83-eb37-45ab-ad19-24b9724dedd6",
   "metadata": {},
   "source": [
    "#### **Task 3.1 - Histograms of surface attributes** - 1 mark\n",
    "This question is based on the dataset loaded in Task 1.\n",
    "\n",
    "**Task deliverable**:\n",
    "- Identify the 14 attributes representing areas in square feet in the dataset, and plot them as histograms.\n",
    "\n",
    "**Remark**: Having a look in the data dictionary provided with the dataset might help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fdf3c9-56b6-4115-b14c-d1de8f6747fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5fb19-0035-4d47-9e7e-a76a1fa6953c",
   "metadata": {},
   "source": [
    "#### **Task 3.2: Correlations with target** - 1 mark\n",
    "**Task deliverable**:\n",
    "- Display in descending order the correlation coefficients between the 14 area attributes identified previously and the target attribute `SalePrice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a43a4b-ea07-452a-9f2a-79a92a6e2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e4cbd-0fb4-4f99-8823-fa1bb2dff4c6",
   "metadata": {},
   "source": [
    "### **Task 4: Splitting the data in train and test sets** - 4 marks\n",
    "The dataset has a modest size, so it might be worth investigating the use of stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f831e1-8c2f-4799-98fc-b893d0c852dd",
   "metadata": {},
   "source": [
    "#### **Task 4.1: Selecting and preparing an attribute for the sampling** - 2 marks\n",
    "\n",
    "**Task deliverables**:\n",
    "\n",
    "1. Select the area attribute (from Task 3.2) with the highest correlation to the target `SalePrice`. \\\n",
    "Based on the selected attribute, sort the instances into 4 bins. The bins edges are defined as: [$0$, $median \\times 0.5$, $median$, $median \\times 1.5$, $\\infty$]. \\\n",
    "$median$ corresponds to the median value of the previously selected area attribute's values. \\\n",
    "Create a new attribute corresponding to the category each instance (house) is binned into.\n",
    "3. Visualise the number of instances (houses) in each category with a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cdeb6-f1e0-4744-ae6d-70404b9a12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab91b6-bb5c-4818-b981-f6618f1f5cc9",
   "metadata": {},
   "source": [
    "#### **Task 4.2: Train/valid/test sets split using stratified sampling** - 2 marks\n",
    "\n",
    "**Task deliverable**:\n",
    "\n",
    "1. Split the dataset into train, validation and test sets using stratified sampling based on the the new categorical attribute you created in Task 4.1. Use a 60%/20%/20% split for train/validation/test sets.\n",
    "2. Compare the proportions of instances (in percentage) for each category in:\n",
    "   - the overall dataset,\n",
    "   - the stratified train and test sets, and\n",
    "   - randomly splitted train and test sets (without stratified sampling).\n",
    "   \n",
    "   Briefly explain the difference of results between stratified and random sampling. Does the choice of stratified sampling make sense in that case?\n",
    "\n",
    "**Remark**: \n",
    "- Don't forget to drop the new feature created for stratifed sampling from the training and test sets after completing this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96642f-2bee-4263-aa65-56995a29eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388037ad-51aa-4a07-add3-c0c124b636ae",
   "metadata": {},
   "source": [
    "### **Task 5: Pre-processing pipeline** - 4 marks\n",
    "\n",
    "**Task deliverables**:\n",
    "1. Build a pre-processing pipeline with:\n",
    "    - Missing values handling based on median for numerical attributes and most frequent value for categorical attributes.\n",
    "    - Standardisation for numerical attributes.\n",
    "    - Categorical attributes encoding with one-hot encoding.\n",
    "\n",
    "2. *Fit and apply* the pipeline to the stratified train set, without the target values (`SalePrice`). Then, *apply* the fitted pipeline to the stratified validation and test sets, without the target values. \n",
    "\n",
    "3. Did the number of attributes in the sets change after applying this pre-processing pipeline? If yes, briefly explain why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4395b8-1e71-4359-8bd2-5f0ca4669729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d0df0-3a86-4414-a882-34492e5527ca",
   "metadata": {},
   "source": [
    "### **Task 6: Feature selection with Mutual information** - 4 marks\n",
    "\n",
    "The previous step led to generating a lot of attributes and not all of them might be useful to build a model. \n",
    "In this task, you have to perform feature selection using mutual information. Mutual information can be used to measure associations between an attribute and the target. It is similar to correlation in this way, but the advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.\n",
    "\n",
    "**Task deliverables**:\n",
    "\n",
    "1. Using the train set only, generate a ranking of the attributes based on mutual information with the target `SalePrice`.\n",
    "2. Drop the attributes with mutual information smaller than 0.01 from the train, valid and test sets.\n",
    "3. Display the number of attributes which were removed during the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f6cda-34e3-4e6b-b5f4-9b4f79dcfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac759a-962a-4e03-b199-4bf74d8b19f2",
   "metadata": {},
   "source": [
    "## **Part 2: Training a simple neural network** - 13 marks\n",
    "\n",
    "This second part is less guided than the first one. You are free to explore a bit more and be more creative to produce the deliverables, within a few constrains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f3500-bfab-4303-a75f-776f51d35869",
   "metadata": {},
   "source": [
    "#### **Task 1: Build and train a neural network** - 8 marks\n",
    "**Task deliverables**:\n",
    "1. Build a simple neural network with 3 hidden layers and 1 output layer. There is no constrain on the number of neurons in each hidden layer, and the activation functions, but you can start with the advice below.\n",
    "2. Build a training loop to train your model on the training set you produced in Part 1. Your training loop should:\n",
    "   - Display the training loss and validation loss after each epoch.\n",
    "   - Store the values of the training loss and validation loss after each epoch. \n",
    "3. Display a plot of your training and validation losses and demonstrate that your model is learning. \n",
    "4. Test your trained model by predicting a few \"new\" instances of the test set and comparing the predicted house value with the expected ones.\n",
    "\n",
    "\n",
    "Constrains:\n",
    "- You are required to use PyTorch for this task.\n",
    "- You are required to use MSE as loss. \n",
    "\n",
    "Some advice:\n",
    "- You can start with 150, 75 and 20 neurons in the 3 hidden layers.\n",
    "- You can use ReLu as activation functions between layers to start with.\n",
    "- Try using the Adam optimiser if your network does not train well with SGD.\n",
    "- It might be practical to create a function containing your training loop, as shown in the lectutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236e244-3ce4-4538-a3de-caf5ebf960ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad4124-42ab-440c-a4c2-3dd2a89d436f",
   "metadata": {},
   "source": [
    "#### **Task 2: Report on your model** - 5 marks\n",
    "\n",
    "**Task deliverables**:\n",
    "- Write a brief report (max. 300 words), answering the following questions:\n",
    "    - What do you think about the performance of your model training in Task 1?\n",
    "    - What could you try to do to improve the predictive performance of your model?\n",
    "    - What did you find the most challenging when training the model and how did you solve it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284a841-70ef-47d7-a6ec-f99672ccaaba",
   "metadata": {},
   "source": [
    "Write your report here (max. 300 words)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:COMPSCI714]",
   "language": "python",
   "name": "conda-env-COMPSCI714-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
