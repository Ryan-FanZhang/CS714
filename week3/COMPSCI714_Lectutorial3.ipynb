{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecccb374-c987-43ea-b47d-da44582877d9",
   "metadata": {},
   "source": [
    "# Lectutorial 3: Design choices and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254a8b7-72bd-40d5-84c3-9ecafdce6581",
   "metadata": {},
   "source": [
    "1. Activation functions\n",
    "2. Weights initialisation\n",
    "3. Regularisation\n",
    "4. Hyper-parameter tuning with Optuna\n",
    "5. Learning rate schedules\n",
    "6. Batch-norm\n",
    "7. Faster optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0205b34-e990-4828-9fbc-7723dbb94deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e8a28-0c78-4e5e-ad08-19b6b9b0e5f6",
   "metadata": {},
   "source": [
    "## Coding Time 1: Activation functions, weights initialisation & regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e576fc3-cead-4ed8-9bba-1cbffd60648a",
   "metadata": {},
   "source": [
    "### 1. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193048b7-1092-4603-b27f-5fff900032c3",
   "metadata": {},
   "source": [
    "Let's train a small multi-layer neural network (also called multi-layer perceptron or MLP) on the california housing dataset and try out different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aceaa-c0a7-4d0d-90a0-aabfaa267a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058cd3f-3ad3-42ea-bbc5-90076a83beca",
   "metadata": {},
   "source": [
    "#### Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e333e-338c-4247-a5a1-660579768242",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759959f-33e2-4330-a6da-00bf10f7ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(housing_dataset.data, housing_dataset.target, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_valid = torch.FloatTensor(y_valid)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed47adb-23f3-46b2-90a1-0d87375cb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds\n",
    "X_test = (X_test - means) / stds\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_valid = y_valid.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da15b3d-7de4-416d-b077-374162f47e8a",
   "metadata": {},
   "source": [
    "#### Defining and training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952bb27-492d-446c-a82b-d0d347805288",
   "metadata": {},
   "source": [
    "First we create a class to instantiate model with a simple architecture using 2 hidden layer with ReLU activation functions. As we are doing regression, we can leave the output layer as a linear activation (already applied in the `nn.Linear` layer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdad7dd-532f-4316-89e8-a6d9c005f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_ReLU(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_outputs):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(n_inputs, n_hidden1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden1, n_hidden2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_hidden3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden3, n_outputs)\n",
    "          )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999f8dc-e6b8-4909-8ee0-78ad964e6dd6",
   "metadata": {},
   "source": [
    "Training loop returning training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98483c86-50d7-4e17-aa27-b072c58c04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            y_train_pred = model(X_train_batch)\n",
    "            train_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(mean_epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for X_valid_batch, y_valid_batch in valid_loader:\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(device), y_valid_batch.to(device)\n",
    "                y_valid_pred = model(X_valid_batch)\n",
    "                valid_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        mean_epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(mean_epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}\")\n",
    "\n",
    "    return (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a5a76-e72b-4339-9716-1e8a14146d18",
   "metadata": {},
   "source": [
    "Conversion of the training and validation sets to `TensorDataset` and creation of the dataloader to use mini-batch gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228bf09-e48c-412d-97e2-49e14295012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f769dc7-28c8-496b-a31e-05476225d051",
   "metadata": {},
   "source": [
    "Instanciation of a model from the previously declared class `MLP_ReLU` and selection of the learning rate, optimiser, loss (MSE) and number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe7fa2-757a-468a-b788-42351fe51d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c5d87-d936-4048-a7e0-520209815902",
   "metadata": {},
   "source": [
    "Model training and storage of the trainig and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948921a-ec0d-45d8-9a43-a8c99a89a57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_relu, valid_losses_relu = train(model, optimizer, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f2acb-310e-4c26-93bc-50c1bcf15619",
   "metadata": {},
   "source": [
    "Plot the train and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25510918-0e2b-41bc-924d-b329e499c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses_relu, label = \"ReLU Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc8d46-84e3-432a-8697-efed421a787a",
   "metadata": {},
   "source": [
    "**TODO**: Let's now create a new class `MLP_Sigmoid` to create similar models but with Sigmoid activation instead of ReLU. Complete the following class definition by adding the Sigmoid activation function for the 2 hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac1e90-14f6-40a1-89bf-f5aa76d2b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Sigmoid(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_outputs):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_inputs, n_hidden1),\n",
    "            # TODO\n",
    "            nn.Linear(n_hidden1, n_hidden2),\n",
    "            # TODO\n",
    "            nn.Linear(n_hidden2, n_outputs)\n",
    "            # TODO\n",
    "            nn.Linear(n_hidden2, n_hidden3),\n",
    "            )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e73bbf-18c1-493c-8514-6ded8bc73929",
   "metadata": {},
   "source": [
    "**TODO**: \n",
    "- Train a model instantiated from the `MLP_Sigmoid` class, using the same other settings (learning rate; optimiser, loss and number of epochs) than previously.\n",
    "- Plot the training and validation losses for the model using ReLU and the model using Sigmoid on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bbcee-050a-4d0c-88f7-822c881a0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276c010-0a90-42fc-8617-431b5e996757",
   "metadata": {},
   "source": [
    "**TODO**: Have a look at the list of activation functions available in PyTorch and try out another one (e.g., Leaky ReLU). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c45518-8912-47ad-98b1-7482059eec43",
   "metadata": {},
   "source": [
    "#### Output activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200719d-0d30-4017-ba12-6b21f33c1e8e",
   "metadata": {},
   "source": [
    "The output activation function depends on the problem you are trying to solve:\n",
    "- Regression: Linear activation with MSE, RMSE or MAE loss.\n",
    "- Binary/Multilabel classification: Sigmoid with Binary Cross Entropy Loss.\n",
    "- Multi-class classification: Softmax with Cross Entropy Loss.\n",
    "\n",
    "**Important**: \n",
    "When doing binary/multilabel classification, either:\n",
    "- Apply a `nn.Sigmoid()` activation function to your output layer and use `nn.BCELoss` as loss function, OR\n",
    "- Do not apply `nn.Sigmoid()` activation function to your output layer and use `nn.BCEWithLogitsLoss` as loss function (applies sigmoid before BCE loss).\n",
    "  In that case, you also will need to apply sigmoid to the output of the model at inference timle (after training).\n",
    "\n",
    "When doing multi-class classification: \n",
    "- Do not apply `nn.Softmax()` activation function to your output as it is already applied in `nn.CrossEntropyLoss`.\n",
    "- Apply the softmax function at inference time (after training). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c026e-cdab-4a27-afef-5e0edf1a0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTILABEL CLASSIFICATION 1 (also works for Binary classification)\n",
    "# Either Sigmoid + BCELoss\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_classes):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(n_inputs, n_hidden1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden1, n_hidden2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_hidden3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_classes)\n",
    "          nn.Sigmoid()\n",
    "      )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n",
    "\n",
    "model = MultiLabelClassifier(n_inputs=10, n_hidden1=50, n_hidden2=40, n_classes=5)\n",
    "criterion = nn.BCELoss()  # Do not Automatically applies sigmoid\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe46896-0eaf-4b0b-b67b-f7c16d2355f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTILABEL CLASSIFICATION 2 (also works for Binary classification)\n",
    "# Or No Sigmoid + BCEWithLogitsLoss\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_classes):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(n_inputs, n_hidden1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden1, n_hidden2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_hidden3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_classes)\n",
    "          # No nn.Sigmoid here\n",
    "      )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n",
    "\n",
    "model = MultiLabelClassifier(n_inputs=10, n_hidden1=50, n_hidden2=40, n_classes=5)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Automatically applies sigmoid\n",
    "# ...\n",
    "\n",
    "# At inference time\n",
    "new_input = ... # Some new input\n",
    "prediction = torch.sigmoid(model(new_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b4d67-0e56-43ef-8102-25749672ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-CLASS CLASSIFICATION \n",
    "\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_classes):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(n_inputs, n_hidden1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden1, n_hidden2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_hidden3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden2, n_classes)\n",
    "          # No nn.Softmax here\n",
    "      )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n",
    "\n",
    "model = MultiLabelClassifier(n_inputs=10, n_hidden1=50, n_hidden2=40, n_classes=5)\n",
    "criterion = nn.CrossEntropyLoss()  # Automatically applies softmax\n",
    "# ...\n",
    "\n",
    "# At inference time\n",
    "new_input = ... # Some new input\n",
    "prediction = torch.softmax(model(new_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67667f-5dd6-47d7-a4da-cb0c8f6fb0c5",
   "metadata": {},
   "source": [
    "## 2. Weights initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e5a55-8c2e-4c98-a577-17f93c281b61",
   "metadata": {},
   "source": [
    "The way the weights are initialised plays an important role to allow the signal to \"flow\" properly during the forward pass and the backpropagation of the gradients. I.e., we want to avoid values of activations and gradients to become too large or too small. This can be achieved by controling the variance or the bonds of the distribution from which the weights' values are drawn from. This helps to maintain a similar variance of ativations across the different layers and mitigate vanishing or exploding gradients.\n",
    "\n",
    "There are two main initialisations used for the weights: \n",
    "- Xavier/Glorot initialisation for linear, tanh, sigmoid and softmax activation functions.\n",
    "- Kaiming/He initialisation for ReLU and its variants (mitigate Dying ReLU problem).\n",
    "\n",
    "Weights's values can be drawn from a normal or uniform distribution with variance/bounds calculated based on the number of neurons in the previous and next layer for Xavier/Glorot, or only based on the previous layer for Kaiming/He. \n",
    "\n",
    "Usually, drawing weigths' values from a uniform distribution is prefered for smaller networks, while drawing them from a normal distribution is prefered from deeper networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc11543-aa69-44b3-aabd-21d1735be52d",
   "metadata": {},
   "source": [
    "The `nn.init` module contains several types of initialisation methods, including Xavier/Glorot and Kaiming/He uniform and normal. \n",
    "\n",
    "The following functions takes a module (i.e., a model) as input and apply Kaiming/He uniform or normal initialisation to the weigths. The biases are initialised to 0. It is acceptatble as the biases are not contributing to the symmetry problem as weigths do. Their primary role is to provide an adjustable threshold for neuron activation, so they can start from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b9298-1338-47aa-af58-9b91d3195ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_he_uniform_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e02e0-a83e-4578-9706-dae994a61956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_he_normal_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight)\n",
    "        nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788e02d-b428-43d2-9eb9-f6e0ad1d07b9",
   "metadata": {},
   "source": [
    "**TODO**: Create similar functions for Xavier/Glorot initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c010e-d0ed-4d45-94ef-fbdf5478ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e4e5e-9c62-44f8-b967-bca491fbba11",
   "metadata": {},
   "source": [
    "You can then use them after instantiating a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3090e0-cdd1-4ed8-8a5b-febacfc281a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "model.apply(use_he_uniform_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf2287-a038-4b70-b517-d3ceed10a384",
   "metadata": {},
   "source": [
    "## 3. Regularisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd1e8-f883-47fe-962e-cb9220eb3be9",
   "metadata": {},
   "source": [
    "#### 3.1. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8060d3-ac1d-4326-b772-131fd22aa752",
   "metadata": {},
   "source": [
    "Let's add some dropout in the input and hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ede00-7d2d-49d2-842c-a4ecbd5a5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Dropout(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_outputs):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "            nn.Dropout(p=0.2), nn.Linear(n_inputs, n_hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2), nn.Linear(n_hidden1, n_hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2), nn.Linear(n_hidden2, n_hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2), nn.Linear(n_hidden3, n_outputs)\n",
    "            )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e99e5-8a02-49a3-aa0c-a8eadb36ff37",
   "metadata": {},
   "source": [
    "*Remark*: As dropout is behaving differently between training and evaluation, it is crutial to include the `model.train()` and `model.eval()` instructions in your training loop, to switch the model in training and evalaution mode, as we have already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7a511-8275-4049-9375-042e4e9532b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_Dropout(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3=50, n_outputs=1).to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccef7ab-45d5-477c-9092-1d367fd88349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_drop, valid_losses_drop = train(model, optimizer, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee6287-09c2-42b4-890c-2dfe1f6ccb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses_relu, label = \"ReLU no dropout Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU no dropout Valid Loss\")\n",
    "plt.plot(train_losses_drop, label = \"ReLU dropout Train Loss\")\n",
    "plt.plot(valid_losses_drop, label = \"ReLU dropout Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff8a48-f427-4725-86e6-6f6147c578e3",
   "metadata": {},
   "source": [
    "**TODO**: Looking at the train and validation losses with dropout activated can be misleading. What do you observe? Does this makes sense given we are using dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e1b37-d5be-4456-948a-ca1c13e5fa28",
   "metadata": {},
   "source": [
    "The effect of dropout as regularisation is not so visible as our network and task are not too complex, with not much overfitting. However, using dropout can lead to higher validation performance (i.e., lower validation loss) with deeper networks and more complex problems prone to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359f0c1-ed4c-45ca-b395-0786801f1478",
   "metadata": {},
   "source": [
    "#### 3.2. L1 and L2 regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0c83d-acf1-48b2-9d13-1e8f6251944e",
   "metadata": {},
   "source": [
    "To implement L1 and L2 regularisation, you need to modify the loss in the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbda55-40f9-489a-b2ac-494ad1610706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reg(model, optimizer, loss_fn, train_loader, valid_loader, n_epochs, reg_type = \"l1\", alpha_reg = 1e-4):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    params_to_regularize = [param for name, param in model.named_parameters() if not \"bias\" in name and not \"bn\" in name]\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            if reg_type == \"l1\":\n",
    "                # l1 Regularisation\n",
    "                main_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "                l1_loss = sum(param.abs().sum() for param in params_to_regularize)\n",
    "                train_loss = main_loss + alpha_reg * l1_loss\n",
    "            else:\n",
    "                # l2 Regularisation\n",
    "                main_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "                l2_loss = sum(param.pow(2.0).sum() for param in params_to_regularize)\n",
    "                train_loss = main_loss + alpha_reg * l2_loss\n",
    "            \n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(mean_epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for X_valid_batch, y_valid_batch in valid_loader:\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(device), y_valid_batch.to(device)\n",
    "                y_valid_pred = model(X_valid_batch)\n",
    "                valid_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        mean_epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(mean_epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}\")\n",
    "\n",
    "    return (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf3c3e-edf4-43fb-a4e9-f76e6b46455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca500d7c-117c-40e8-9b48-f5c8249dec12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses_l2, valid_losses_l2 = train_reg(model, optimizer, mse, train_loader, valid_loader, n_epochs, \"l2\", 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629374e-b0e8-4974-9ad7-367f509d4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses_relu, label = \"ReLU no L2 reg Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU no L2 reg Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses_l2, label = \"ReLU L2 reg Train Loss\")\n",
    "plt.plot(valid_losses_l2, label = \"ReLU L2 reg Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99b38d-688c-4244-8d50-fe9d8339324e",
   "metadata": {},
   "source": [
    "#### 3.3. Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc460c2f-6e9c-48b9-a7c1-87eb180c808f",
   "metadata": {},
   "source": [
    "Below is a simple implementation of early stopping i nthe training loop. It simply stops the training if the validation loss for an epoch is higher than in the previous epoch, and reverts the model to the model from the previous epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a38056-3e7d-4d5c-91e1-a22eca4115ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, optimizer, loss_fn, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            y_train_pred = model(X_train_batch)\n",
    "            train_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(mean_epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for X_valid_batch, y_valid_batch in valid_loader:\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(device), y_valid_batch.to(device)\n",
    "                y_valid_pred = model(X_valid_batch)\n",
    "                valid_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        mean_epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch == 0:\n",
    "            previous_loss = mean_epoch_valid_loss\n",
    "            previous_model = deepcopy(model)\n",
    "            valid_losses.append(mean_epoch_valid_loss)\n",
    "        else: \n",
    "            if mean_epoch_valid_loss > previous_loss:\n",
    "                model = deepcopy(previous_model)\n",
    "                print(f\"Training stopped after epoch {epoch + 1}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}.\")\n",
    "                break\n",
    "            else: \n",
    "                previous_loss = mean_epoch_valid_loss\n",
    "                previous_model = deepcopy(model)\n",
    "                valid_losses.append(mean_epoch_valid_loss)\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}\")\n",
    "\n",
    "    return (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c74121-978d-4742-9d35-2c6b053a686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa393085-9b03-4348-a536-f352711a5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_early_stop, valid_losses_early_stop = train_with_early_stopping(model, optimizer, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d0c36-844f-4b69-82be-183e1f6ad9ae",
   "metadata": {},
   "source": [
    "This basic version of early stopping might be to rigid and you might want to add a `patience` parameter to control the number of epochs without progress in validation loss seen before stopping the training. \n",
    "\n",
    "You can find a more advanced implementation of early stopping [here](https://www.geeksforgeeks.org/how-to-handle-overfitting-in-pytorch-models-using-early-stopping/).\n",
    "The PyTorch Ignite and Lightning libraries also offer a more ready to use `EarlyStopping` class, but they are based on a different high-level API specific to these libraries. Do not hesitate to explore these, with ChatGPT as an assitant!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68772bb-5be5-4aa9-9d11-b4833328c5bd",
   "metadata": {},
   "source": [
    "## Coding time 2\n",
    "\n",
    "- Hyperparameter tuning with Optuna\n",
    "- Learning rate schedule\n",
    "- Batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56288e-5ef7-42c3-8a4e-e953e1b599c5",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter tuning with Optuna\n",
    "\n",
    "Several dedicated libraries can be used for hyperparamter tuning with PyTorch models. For example:\n",
    "- [Optuna](https://optuna.org/)\n",
    "- [Ray Tune](https://docs.ray.io/)\n",
    "- [Hyperopt](https://hyperopt.github.io/hyperopt/)\n",
    "\n",
    "We will see an example with using Optuna.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382c3ef-6cc6-4f85-b762-ebe710221037",
   "metadata": {},
   "source": [
    "We first need to create a function that takes a `Trial` object and use it to have Optuna suggest hyperparameter values to try, train and evaluate a model with the selected hyperparameter values, and return the corresponding validation performance using a given evaluation metric. \n",
    "\n",
    "We start by trying to optimise the learning rate value (float) and the number of neurons in each hidden layer (int). We use the MSE loss as evaluation metric (for classification, we could use accuracy or another target metric).\n",
    "\n",
    "Note: it is also possible to tune categorical hyperparameters with Optuna, using the `suggest_categorical()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d25e76-df03-4113-8f45-5f1a6cc03a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_loader, valid_loader):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True) # Suggest a float value in the chosen range\n",
    "    n_hidden1 = trial.suggest_int(\"n_hidden1\", 100, 300) # Suggest an int value in the chosen range\n",
    "    n_hidden2 = trial.suggest_int(\"n_hidden2\", 50, 200) # Suggest an int value in the chosen range\n",
    "    n_hidden3 = trial.suggest_int(\"n_hidden3\", 20, 100) # Suggest an int value in the chosen range\n",
    "    model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_hidden3=n_hidden3, n_outputs=1).to(device)\n",
    "    mse = nn.MSELoss()\n",
    "    n_epochs = 30\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    _ , valid_losses = train(model, optimizer, mse, train_loader, valid_loader, n_epochs)\n",
    "    return valid_losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d839-370e-493a-a36c-eec4c7516102",
   "metadata": {},
   "source": [
    "Then, we need to create `Sampler` object specifying the algorithm we want to use for the optimisation. Some algorithms might select purely random values at each trial, some will use past information to guide the search. You can have a look at the different Optuna sampler algorithm options in the [documentation](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html).\n",
    "\n",
    "Then, we need to create a `Study` object and pass the sampler as an argument, as well as the `direction` of the optimisation. In our case, we want to minimise the MSE used as evalaution metric. \n",
    "\n",
    "Finally, we can start the hyperparameter tuning by calling the `optimize` method and passing the objective function and number of trial as arguments. The number of trials correspond to the number of hyperparameter combinations the algorithm will try. The highest this number is, the more chance you have to find good hyperparameter values, but the longer it will take.\n",
    "\n",
    "To pass the training and validations sets loader to the objective function, we can use the `functools.partial()` function which allows to provide default values for the objective fucntion arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff7083-b29e-43a5-ab44-4ed071db7fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler() # Use the Tree-structured Parzen Estimator algorithm for the optimisation\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "objective_with_data = partial(objective, train_loader=train_loader, valid_loader=valid_loader)\n",
    "study.optimize(objective_with_data, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cfdf3-2714-4533-945d-f5219fdceac3",
   "metadata": {},
   "source": [
    "The best hyperparameter values are found in `study.best_params` and the corresponding best evaluation metric value in `study.best_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f3040-8b6d-4d6d-9b7a-42557ac575ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c867c-4e2c-4333-b367-6d0387690482",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515ebcf-235b-4279-8ebb-5764bdd920db",
   "metadata": {},
   "source": [
    "**TODO**: Try to let it run for longer and see if it can find significantly better values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638956a0-4a45-4867-80a7-0bddd38651ce",
   "metadata": {},
   "source": [
    "To speed up the process, you can use a `Pruner` to stop trials that are leading nowehere (i.e., whose performance is below previously seen trials). For example, the `MedianPruner` will stop a trial if its performance is below the median of previous trials. The pruner needs to warmup first over a couple of trials set with the `n_startup_trials` parameter (5 by default). After that, it will monitor the performance every few epoch (set with the `interval_steps` parameter), after a few first warmup epoch (set with the `n_warmup_steps` parameter). For each of the controlled epochs, it verifies if the perfomance is better than the median perfomance at the same epoch for previous completed trials. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b33c8-7dde-462b-be7f-ce21ff1e22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0, interval_steps=1)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726c262-6095-4432-a0dc-22717c915375",
   "metadata": {},
   "source": [
    "To use the pruner, you need to pass information about the validation performance after each epoch of training during a trial. In our case, we need to modify the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a3dff-9ee5-42b5-8907-e4970b4c3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_pruner(model, optimizer, loss_fn, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            y_train_pred = model(X_train_batch)\n",
    "            train_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(mean_epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for X_valid_batch, y_valid_batch in valid_loader:\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(device), y_valid_batch.to(device)\n",
    "                y_valid_pred = model(X_valid_batch)\n",
    "                valid_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        mean_epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(mean_epoch_valid_loss)\n",
    "\n",
    "        # Pass information to Optuna about the validation performance for the last epoch\n",
    "        trial.report(valid_losses[-1], epoch)\n",
    "        # Raise an exception to stop the training if Optuna's pruner decides to prune the current trial\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}\")\n",
    "\n",
    "    return (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e3b33-0ed7-4dda-9451-8abebd98b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_loader, valid_loader):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True) # Suggest a float value in the chosen range\n",
    "    n_hidden1 = trial.suggest_int(\"n_hidden1\", 100, 300) # Suggest an int value in the chosen range\n",
    "    n_hidden2 = trial.suggest_int(\"n_hidden2\", 50, 200) # Suggest an int value in the chosen range\n",
    "    n_hidden3 = trial.suggest_int(\"n_hidden3\", 20, 100) # Suggest an int value in the chosen range\n",
    "    model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_hidden3=n_hidden3, n_outputs=1).to(device)\n",
    "    mse = nn.MSELoss()\n",
    "    n_epochs = 30\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    _ , valid_losses = train_for_pruner(model, optimizer, mse, train_loader, valid_loader, n_epochs) # NEW: Changed the train_for_pruner to use for pruning\n",
    "    return valid_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c801836-50fd-4bc5-a6b0-853d0085ecec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study.optimize(objective_with_data, n_trials=10) # Runs 5 trials without pruning and then activates the pruning until reaching 10 trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6078f84-84c0-4bfa-9237-6b9015e42427",
   "metadata": {},
   "source": [
    "You can read more about Optuna range of pruners in the [documentation](https://optuna.readthedocs.io/en/stable/reference/pruners.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d004d87-cabb-4f7e-8a34-7ecfc49f187c",
   "metadata": {},
   "source": [
    "### 2. Learning rate schedule\n",
    "\n",
    "The `torch.optim` module contains a sub-module called `lr_scheduler` which allows you to use several types of schedulers.\n",
    "\n",
    "First, we need to modify the training loop to accept a scheduler as parameter, and update the scheduler (i.e., the learning rate) during the training, in a similar fashion we update the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9d311-fe30-4b48-afc3-a5cb88a9a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_scheduler(model, optimizer, scheduler, loss_fn, train_loader, valid_loader, n_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            y_train_pred = model(X_train_batch)\n",
    "            train_loss = loss_fn(y_train_pred, y_train_batch)\n",
    "            epoch_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mean_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(mean_epoch_train_loss)\n",
    "\n",
    "        # Scheduler update for every epoch (can be moved inside epoch loop for updates at every batch)\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step() # Take a step of scheduler at the end of the epoch\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for X_valid_batch, y_valid_batch in valid_loader:\n",
    "                X_valid_batch, y_valid_batch = X_valid_batch.to(device), y_valid_batch.to(device)\n",
    "                y_valid_pred = model(X_valid_batch)\n",
    "                valid_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "                epoch_valid_loss += valid_loss.item()\n",
    "        mean_epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
    "        valid_losses.append(mean_epoch_valid_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Training Loss: {mean_epoch_train_loss:.4f}, Valid Loss: {mean_epoch_valid_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch  + 1}: SGD lr {before_lr} -> {after_lr}\")\n",
    "\n",
    "    return (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1638a-b054-47dd-b553-528deabf6825",
   "metadata": {},
   "source": [
    "We then need to instantiate a scheduler before training and \"link\" it to the optimiser.\n",
    "\n",
    "Below, we use a linear scheduler which will decay the learning rate linearly from the original leanring rate value to 50% of its original value over 20 epochs. After 20 epochs, the learning rate will remain constant at 50% of its original value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbbdfb-6341-4292-ae41-cf55c066530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "n_epochs = 30\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=20)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2, eta_min=0.001)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=n_epochs) # Need to move scheduler update in epoch loop\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4e4b9-fc63-4353-9a92-f79eda10a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_lr_sched, valid_losses_lr_sched = train_with_scheduler(model, optimizer, scheduler, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d6889-c619-478d-9493-3123f4f3b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses_relu, label = \"ReLU no lr schedule Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU no lr schedule reg Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses_lr_sched, label = \"ReLU lr schedule Train Loss\")\n",
    "plt.plot(valid_losses_lr_sched, label = \"ReLU lr schedule Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf8cb2-2ac5-4d9e-ba48-4821e7ef517d",
   "metadata": {},
   "source": [
    "PyTorch provide a lot of different learning rate schedules you can use. Some examples are shown in [this Kaggle notebook](https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling). \n",
    "\n",
    "**TODO**: Try to use other learning rate schedules and see the impact on training. Some schedules alternate increasing and decreasing the learning rate to try to escape local optimum (e.g., `CosineAnnealingLR` or `OneCycleLR`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef3432-7594-462f-a2d6-5ebf2c497e7c",
   "metadata": {},
   "source": [
    "### 3. Batch-normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326fb77-e5e6-4f81-ace8-4adb75bbfa14",
   "metadata": {},
   "source": [
    "Let's include batch normalisation layers in our network. The authors of the method advocate to place them before the activation function of each layer, but this is a debated issues. In the case below, we placed them after the activation functions as it seems to work better. The choice seems to depend on your network architecture, therefore you can try both and see what works best fro you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28652aa8-8440-41b3-9b3c-ba57a27aa18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_hidden3, n_outputs):\n",
    "      super().__init__()\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(n_inputs, n_hidden1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(n_hidden1),\n",
    "          nn.Linear(n_hidden1, n_hidden2),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(n_hidden2),\n",
    "          nn.Linear(n_hidden2, n_hidden3),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(n_hidden3),\n",
    "          nn.Linear(n_hidden3, n_outputs)\n",
    "          )\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7766b0-a159-4178-9c64-8db008ac45e2",
   "metadata": {},
   "source": [
    "*Remarks*: \n",
    "- As batch normalisation is behaving differently between training and evaluation, it is crutial to include the `model.train()` and `model.eval()` instructions in your training loop, to switch the model in training and evaluation mode, as we have already done.\n",
    "- For models dealing with data in 2 or 3 dimensions (e.g., convolutional neural networks work with 2D images), you can use `nn.BatchNorm2d` or `nn.BatchNorm3d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702df143-86f3-4150-b269-662c479f035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_BN(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6dab7-0b58-4281-9e80-b711b163a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_bn, valid_losses_bn = train(model, optimizer, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8ee88-f568-4407-88d2-03bd44f1d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses_relu, label = \"ReLU no BN reg Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU no BN reg Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses_bn, label = \"ReLU BN reg Train Loss\")\n",
    "plt.plot(valid_losses_bn, label = \"ReLU BN reg Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab73c9-ae89-4715-bf08-f068b38d5712",
   "metadata": {},
   "source": [
    "## Coding Time 3: Faster optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79e31e-8923-4220-975d-2c674a5f19cf",
   "metadata": {},
   "source": [
    "The is a range of optimisers you can use with PyTorch, from the module `torch.optim`. We only used SGD and Adam (to train the image classifier in Lectutorial 2), but you can try a range of other optimisers such as RMSProp or AdaGrad. You can find the list of optimisers available in the `torch.optim` documentation. \n",
    "\n",
    "To change the optmiser, you simply need to pass a different one to your training function. Be mindful that different optimisers might have different hyper-parameters you can tune. For example, you can try to change the betas values for the ADAM optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988239c-6b7d-47a6-8089-f6f25fc1d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_ReLU(n_inputs=X_train.shape[1], n_hidden1=200, n_hidden2=100, n_hidden3 = 50, n_outputs=1).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999)) # Changed to ADAM optimiser\n",
    "mse = nn.MSELoss()\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14d641-cf3c-40bb-b9d8-dbf6ca16cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_adam, valid_losses_adam = train(model, optimizer, mse, train_loader, valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b14d05-c21a-47e1-84a0-f64dbb0d9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses_relu, label = \"ReLU SGD Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU SGD Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_losses_adam, label = \"ReLU Adam Train Loss\")\n",
    "plt.plot(valid_losses_adam, label = \"ReLU Adam Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3309800-9b01-4649-8a3e-e1c318be3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses_relu, label = \"ReLU SGD Train Loss\")\n",
    "plt.plot(valid_losses_relu, label = \"ReLU SGD Valid Loss\")\n",
    "plt.plot(train_losses_adam, label = \"ReLU Adam Train Loss\")\n",
    "plt.plot(valid_losses_adam, label = \"ReLU Adam Valid Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac015822-1003-46eb-a6fa-16ee87ce55c1",
   "metadata": {},
   "source": [
    "**TODO**: Try other optimisers and see how they behave with this model and problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:COMPSCI714]",
   "language": "python",
   "name": "conda-env-COMPSCI714-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
