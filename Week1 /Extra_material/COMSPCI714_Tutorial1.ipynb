{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd5af2e-43c8-4373-add5-bed2625d5df3",
   "metadata": {},
   "source": [
    "# S1 2024 COMPSCI 714 - Tutorial 1: Loading, exploring and pre-processing data\n",
    "\n",
    "Welcome to Tutorial 1! This tutorial covers basics of how to load datasets from files or repositories, explore the data structure and important characteristics, and some basic data cleaning and pre-processing steps using a tabular dataset.\n",
    "\n",
    "*Disclaimer: a big part of the code and text used in this Notebook is directly reused or adapted from Aurélien Géron's notebook: https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb and his book \"Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow, Ed.3\", more particulary from Chapter 2.*\n",
    "\n",
    "Note: To format text in a Notebook, you need to select \"Markdown\" for the cell type, and use the Markdown syntax. You can find a basic Markdown synthax guide at the following link: https://www.markdownguide.org/basic-syntax/.  \n",
    "You can also find a useful Markdown cheat sheet on the IBM website: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07a1f7-7d74-43d4-8af1-c45ab1df1a27",
   "metadata": {},
   "source": [
    "First of all, we need to import some libraries we will use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "72cf9c5a-22d8-4175-b5b6-de0653df5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Scientific computing library\n",
    "import sklearn # Machine Learning library\n",
    "import matplotlib.pyplot as plt # Visualisation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "606eecf3-5a9d-4407-91f1-92f271e152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used to load and manipulate data\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa4591-4771-41f3-b1b6-08d45f7fc157",
   "metadata": {},
   "source": [
    "In this Tutorial, we will use the **californian districts housing dataset** (available at https://github.com/ageron/data) to go through the different steps involved to load, explore, clean and pre-process the data. This dataset contains tabular data. Note that the exploration, cleaning and pre-processing step will differ depending on the type of data you deal with.\n",
    "\n",
    "The task for this dataset is to predict the median house price in a district based on several attibutes such as the position of the district, the total number of rooms in the district, the housing median age in the district, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cff407-bc10-46ad-b923-d5ccc92a4550",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset\n",
    "\n",
    "Let's create a helper function to load a dataset from a file uploaded from a url. We will use the Californian districts housing dataset available at https://github.com/ageron/data.\n",
    "\n",
    "**Todo:** Have a look at the function, and identify the following processing steps:\n",
    "1. Checking for the compressed tgz file containing the dataset.\n",
    "2. If it not found, create a new directory named \"datasets\", download the tgz file from the url and extract its content into the newly created directory.\n",
    "3. Read the dataset csv file (i.e., load the content in a Panda DataFrame), and return it.  \n",
    "\n",
    "*Tip: Do not hesitate to look at a function/method's documentation to better understand what it does.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "257eb12c-fb03-481f-85d2-8bc29215480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_10924\\3335613790.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  housing_tarball.extractall(path=\"datasets\")\n"
     ]
    }
   ],
   "source": [
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408124bd-b5f8-4116-91ff-1b391d86bb1d",
   "metadata": {},
   "source": [
    "## Step 2: Have a first look at the data\n",
    "\n",
    "Now that the data is loaded in a Panda Data Frame, we can have a first look at how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9b7ea011-7896-497f-a59f-92dd49cecdb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce266292-d8ab-461a-9dea-0b588c6167b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e42572-1f3d-47ef-80e0-66e5bfeca357",
   "metadata": {},
   "source": [
    "**Todo:** How many instances (rows) does this dataset have? What are the different types of attributes (columns) in this dataset?  \n",
    "Notice that for one attribute, the count value is not the same as for the other attributes. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011570b5-60fb-4f36-8204-f714baa5f180",
   "metadata": {},
   "source": [
    "Let's have a closer look at the last attribute `ocean_proximity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b84421-1759-4afb-b1dd-e2f156302486",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb66efc-f84c-4181-8346-9a988f5a727a",
   "metadata": {},
   "source": [
    "We can also get statistics for each numerical attibute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22230ea6-e61c-443c-87fd-59712c252b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8565c7-6629-43c2-bbf1-13e55aca475c",
   "metadata": {},
   "source": [
    "**Todo:** What statistics are calculated? Discuss with your group some insights about this dataset you can derive from these values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63ba01-ca28-47e7-b382-bd1834ee99ad",
   "metadata": {},
   "source": [
    "You can also visualise the distribution of values for each attibute by using histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7dc63c-9763-41c5-b232-4115c2c99377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If needed, you can change the font sizes with the following 5 lines.\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=8)\n",
    "plt.rc('ytick', labelsize=8)\n",
    "\n",
    "# Code to generate and plot the histograms for the numerical attributes\n",
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfcbcb-0f34-4acd-b582-966b716df7d8",
   "metadata": {},
   "source": [
    "**Todo:** What observations can you make about the data when looking at these histograms? Look at the form of the distributions, the scale of the axis, and if you can notice any odd paterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ad28e-b32f-4766-bac5-403f3230d2c8",
   "metadata": {},
   "source": [
    "Once you have discussed your observations with your classmates, you can run the following cell to reveal some insights and compare them with what you discussed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f3fe211-c4a5-4e47-90dd-1bb9d449c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The median income attribute does not look like it is expressed in dollars. When asked, the team that collected the data tells you that 1. the numbers roughly represent tens of thousands of dollars (e.g. 3.0 is approximately $30 000) and 2. The data was scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (median) for lower median incomes. Working with preprocessed attributes is common in machine learning, and it is important to understand how the data was computed.\n",
      "\n",
      "- The housing median age and the median house value were also capped (see the large bin at the end of the distribution). The cap on the median house value might be a serious problem as this is the target we want to predict. The model might learn that the house value never goes higher than $500 000, while this last bin might include house values higher than $500 000. In that case, you need to refer to the task (e.g., check with your client) to see if this is a problem. If the task requires precise predictions beyond $500 000, you will either need to collect more precise labels for the values that were capped, or you can decide to remove the capped instances from the dataset (in that case, your model might not perform as well for house values greater than $500 000. \n",
      "\n",
      "- The attributes have different scales (look at the y axis). This might require some feature scaling/normalisation. \n",
      "\n",
      "- The distibution of the values for several attributes is skewed (not centred on the median). This might make the training of models harder as it makes it harder to detect a \"typical\" pattern. This might require some transformations to have a more symmetrical / Gaussian shape distribution for these attributes. \n"
     ]
    }
   ],
   "source": [
    "def read_insights(file):\n",
    "    with open(file, \"r\") as file:\n",
    "        print(file.read())\n",
    "read_insights(\"do_not_read.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e26b1-8e5f-40ed-a0f6-020ebee7c624",
   "metadata": {},
   "source": [
    "## Step 3: Set a test set aside\n",
    "\n",
    "Before exploring the data further, you should set a portion of it aside at this stage. This is called a test set, and it will be used for model evaluation and validation. It is important to set it aside now, because you should not look at your test set at all before the final evaluation of your model. We will discuss this further in the lectures and tutorials next week.  \n",
    "When creating a test set, you can choose one of the two approaches depending on the size of your dataset.\n",
    "- If your dataset is large enough (especially relative to the number of attributes): sample purely randomly from the dataset.\n",
    "- If your dataset is not large and you know that an attribute has a high influence on the target prediction: use stratified sampling to avoid introducing a significant sampling bias. This ensures that your test set has a similar distribution of values for each attribute as the full dataset. Else, the estimated importance of a category of values for a specific attribute might be biased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e821f5-5ff2-45e6-8397-02dcbbfa1ebf",
   "metadata": {},
   "source": [
    "Coming back to the housing dataset, imagine you are told that the median income attribute is very important to predict the median house value. Run the next two cells to create and display an histogram of the median income split into 5 bins. Notice that the first cell creates a new attribute in the Panda Dataframe `housing` called `income_cat`, which corresponds to the category of income the instance (district) is binned into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132c0ab-beb0-4c37-971c-5796e1597ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee652ea2-1b3e-4150-bc7a-6365a1d4dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349a134-72cd-4667-9752-1b060c254488",
   "metadata": {},
   "source": [
    "We can then use the `train_test_split` function from the `sklearn.model_selection` module to perform the train/test set split. We need to specify the test set size (here 20% of the total dataset), and the attribute we want to use as reference for the stratify sampling process (the income category).  \n",
    "Notice that we also specify a random state number. This is to ensure that we always get the same instances in the test set when we run the train/test split several times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55419c3-6034-4217-b330-3915bf9f0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbeb4cc-1dca-40c4-a175-415e2158c873",
   "metadata": {},
   "source": [
    "To visualise the effect of stratified vs random sampling with our dataset, run the following cell to generate a table comparing the sampling bias (error) for the to different approaches, with respect to the income categories in the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb342a-3ba6-4f2a-8095-f7cb5ca5cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall %\": income_cat_proportions(housing),\n",
    "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
    "    \"Random %\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props.index.name = \"Income Category\"\n",
    "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
    "                                   compare_props[\"Overall %\"] - 1)\n",
    "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
    "                                  compare_props[\"Overall %\"] - 1)\n",
    "(compare_props * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09755122-1cbd-45e2-9792-09528b5559b9",
   "metadata": {},
   "source": [
    "Before moving on, we need to remove the attribute *income_cat* that we created for the stratified sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6becb20c-eb52-48e1-acb5-7969eef6ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee516a8-36e8-4c9c-96d8-14459d7c5cb6",
   "metadata": {},
   "source": [
    "Now that we have a test set set aside, we can further explore the data left in the train set that will be used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e634a-eaa7-45fc-8982-1931e181fa57",
   "metadata": {},
   "source": [
    "## Step 4: Explore and visualise the data \n",
    "To gain more insights about the data, which might help us make choices further down the line, we can use more sophisticated visualisations, and also look at correlations in the data.  \n",
    "We will keep the test set aside and use the training set to run further exploration. If you training set is too large, you can sample an exploration set to make the exploration easier and faster.  \n",
    "\n",
    "Fist of all, let's make a copy of the training set, to be able to reverse back to it after making transformations for the exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e20fc-aa98-4e9f-b638-151f3277d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61de24c-5e0c-48d2-af93-a6ff786a5d1d",
   "metadata": {},
   "source": [
    "### Visualisation of geographical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3eecd-6cf8-492a-a540-3313222e5ecc",
   "metadata": {},
   "source": [
    "First, notice that the dataset contains some geographical localisation attributes (latitude and longitude). A first interesting visualisation would be to display a scatterplot of the geographical position of all the districts.  \n",
    "**Todo**: Write a Python instruction the following cell to generate and display such a plot.  \n",
    "Hint: you can use the Panda Dataframe plot() method with the `housing` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632ee32-a750-4f24-8981-e1f934e4e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type an instruction to display a scatterplot of the longitude and latitude attributes of the housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418a66d-13b7-4b66-88d5-03a22b96efce",
   "metadata": {},
   "source": [
    "**Todo**: Does the shape remind you of anything? (if not, you might want to switch the x and y coordinates)  \n",
    "Play around with the options for the visualisation. E.g., set the `alpha` parameter (it regulates the transparency of the plot) so that you have a better idea of the density of points in the plot. Where are most districts situated? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf0442-ea50-4e6c-b981-f326230ff4fc",
   "metadata": {},
   "source": [
    "Once you have complete the previous instruction, you can run the following cell to plot the districts localisation, combined with the population information (radius of the cicle), and the median house value (colour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b81fc-da4c-4155-aef3-1a9f5efe94b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False, figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eebd2b-6aab-4994-818b-a452d2efe606",
   "metadata": {},
   "source": [
    "**Todo**: Discuss some insights you can get from this visualisation. What attributes in the dataset are likely to be influencial on the target (median house value)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19eb71-3e82-4c84-9158-585e9cbef3be",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec17fe-dc88-454a-a84a-acf75b473e25",
   "metadata": {},
   "source": [
    "Next, we can look at the correlations between attributes by calculating the standard correlation coefficent, or Pearson's correlation coefficient.  \n",
    "Run the following code to compute the correlation matrix containing the correlation coefficient between each pair of attributes, and then look at the correlation of each attribute to the target median house value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d98da-8230-4a82-9d21-c6b4367a82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) #Need to set numeric_only to True with Pandas 2.0\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4eabda-1814-42b2-9f55-d004629b83c0",
   "metadata": {},
   "source": [
    "**Todo**: Discuss what insight you can get from these correlation values. Do you see any notable correlations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82f6ed-19c3-4f7d-afdd-e9ccdf137b62",
   "metadata": {},
   "source": [
    "Machine learning algorithms are trying to build models by learning patterns in data. These patterns are linked to correlations between attributes and targets. \n",
    "You can also visualise correlations by using scatter plots.  \n",
    "**Todo**: Complete the code below to use the Pandas `scatter_matrix` function to plot the 3 most correlated attributes with the target median house value, as well as the target attribute. This function plots a matrix of the attributes values against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa41087-ee19-422c-a424-e37db57f1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\"] # Extend this list with the 3 most correlated attributes with the target\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b161d70-ba33-4f93-80be-5483eeacd86b",
   "metadata": {},
   "source": [
    "In such correlation matrix, the main diagonal is usually composed of straight lines as it plots the each attribute against itself. Pandas `scatter_matrix` plots the histogram of the values for this attribute instead, which is a more useful information.  \n",
    "Let's have a closer look at the most correlated attribute, the median income.  \n",
    "**Todo**: Write a Python instruction in the next cell to plot the median house value (y axis) against the median income (x axis), with a low transparency coefficient `alpha` value (e.g., 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c004e5b-0fe0-4681-b438-b4015932a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type an instruction to display a scatterplot of the median income and the median house value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3bc41-3e4e-45e8-a5ce-5cdcdc6168f3",
   "metadata": {},
   "source": [
    "**Todo**: Discuss some observations you can make from this plot.  \n",
    "Once you have discussed a few observations, you can run the next cell to reveal more insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "538d7154-604a-47b5-9cab-001140113dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The correlation is quite strong visually, with a clear upward trend and points not too dispersed. \n",
      "\n",
      "- You can identify the median house value cap with an strong horizontal line at $500 000.\n",
      "\n",
      "- You can also see less visible horizontal lines around $450 000, $350 000, and maybe $280 000, $260 000, etc. These might be linked to some irregularities in the data collection or recording. There may be a risk for your algorithm to pick these misleading trends up, which might be detrimental to your model performance. Therefore, you may want to remove the corresponding data points.\n"
     ]
    }
   ],
   "source": [
    "read_insights(\"do_not_read2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba1ab4-1ed0-4320-8b80-50a677430561",
   "metadata": {},
   "source": [
    "Be mindful that the correlation coefficient only consider linear correlations. There might also be some interesting non-linear correlations in the data, useful for a algorithm to build a model, if it is able to learn non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af3332-0d0c-44e6-aff0-cb94ad37ab94",
   "metadata": {},
   "source": [
    "### Attribute combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da7965-e121-488f-8643-60dab3e4e2dc",
   "metadata": {},
   "source": [
    "A last thing you might want to try is to create combinations of existing attributes and see how they correlate with the target. Some of the existing attributes might not be very interesting by themselves (e.g., the total number of bedrooms in a district), but might give you better insights if combined with other attributes.  \n",
    "**Todo**: Complete the following code to add 3 new attributes to the housing DataFrame:\n",
    "- The number of rooms per household\n",
    "- The bedroom ratio (total number of bedrooms with respect to the number of rooms)\n",
    "- The number of people per household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c476ce-c55b-4933-b618-474348ff6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_house\"] = # Write here\n",
    "housing[\"bedrooms_ratio\"] = # Write here\n",
    "housing[\"people_per_house\"] = # Write here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbcb8d-fd7e-45b1-aa5a-af9986868f08",
   "metadata": {},
   "source": [
    "**Todo**: Once you added the new attributes, run the following cell to calculate the correlation coefficients again and display the correlations with respect to the target median house value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064db58-b568-4df1-8f6b-7c23403ce1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa2381-71be-485a-bb52-2240c9f18129",
   "metadata": {},
   "source": [
    "**Todo**: Are the new features more informative? Discuss extra insights you might get from this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae4a8a-f0b3-48b8-bbda-eb2edff3bb99",
   "metadata": {},
   "source": [
    "You do not have to test everything and explore every aspect of your data during this exploration stage. The main objective is first to verify if there is interesting patterns to be modelled in the data, and then to give you general insights which might help you to make choices for the next steps and training a model later on.  \n",
    "The process of training a model is highly iterative, as we will see again in later stages. You might end up needing to explore your data further after training and evaluating your model a first time, to gain further insights about how to improve it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b064e9-c9a0-4443-8e7d-16588f25b44b",
   "metadata": {},
   "source": [
    "## Step 5: Data cleaning\n",
    "After getting some insights about the data, you need to clean it. Cleaning the data usually involves:\n",
    "- Dealing with missing values in the dataset\n",
    "- Removing outliers\n",
    "\n",
    "Before starting this process, it is best to revert to a clean dataset and seperate the predictive attributes from your target attribute in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eed883-7372-4114-8d75-7e1dd9771b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d08a54-ef1c-445e-bd47-2d0fcd5362c2",
   "metadata": {},
   "source": [
    "Note that `strat_train_set.drop()` creates a copy of `strat_train_set` without the selected column, it doesn't actually modify `strat_train_set` itself, unless you pass `inplace=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c255e-ce9c-498e-9a71-81e7a6d331ad",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "Remember from the data exploration phase that some instances have missing values for the `total_bedrooms` attribute. Most machine learning algorithms do not handle well missing values, so it is generally necessary to fix this before you use the data to train a model. There are three main ways to do so:\n",
    "1. Remove the instances with missing attribute value\n",
    "2. Remove the whole attribute\n",
    "3. Impute the missing value, i.e. replace it with some value such as zero, the mean, the median, etc.\n",
    "\n",
    "You may choose on approach or another depending on your problem, your data, the reason the values are missing, etc. Each approach comes with its own caveats. Removing instances or attributes might result in potentially loosing some valuable information (especially if you have a small dataset with a small number of attributes), while imputing values might introduce some noise to your data.  \n",
    "\n",
    "You can directly use the Pandas DataFrame's `dropna()`, `drop()` and `fillna()` methods to perform repectively instance removal, attribute removal and imputation.  \n",
    "E.g.:  \n",
    "```python\n",
    "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)    # option 1\n",
    "\n",
    "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median()  # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4f3ec-bfc4-4a4b-b4bb-483c10efc8b7",
   "metadata": {},
   "source": [
    "Let's choose to impute the missing values for our case, since it is the least destructive option, and the dataset rather small dataset with few attributes. Instead of using the Pandas DataFrame methods listed above, let's use a the Scikit-Learn `SimpleImputer` class. This is a more sophisticated approach which will allow us to train an imputer able to impute values not only for the training set, but also for the validation set, test set, and for any new data instance fed to the model.  \n",
    "*Note: The validation set is used to validate the choices made when training the model, before evaluating the final version of the model on the test set. We will talk more about it next week.*\n",
    "\n",
    "Run the following cell to create a `SimpleImputer` instance, using the median value of an attribute to replace any missing value for this attribute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0b65d-77b4-450a-9ba8-431e62eb8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95ba80-f6bf-4d4d-9b00-7bb81e9f4602",
   "metadata": {},
   "source": [
    "The median can only be computed for numerical attributes. Therefore, it is necessary to create a copy of the dataset with only the numerical attributes. You can do so by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488da7b4-9c69-4ae8-b9f1-0ee0575ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612f72e-19a8-48ac-88b9-8ea6b4f0e30b",
   "metadata": {},
   "source": [
    "You can now compute the mean for each attribute using the `fit()` method on the `imputer` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ab5a2-6781-4331-92b7-a7d0e44a7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce65ccc-cc46-431e-af51-0b57b6dff623",
   "metadata": {},
   "source": [
    "The median value for each attribute is stored in the `statistics_` field of the `imputer` instance. Run the following two cells to compare the `statistics_` field with the manually computed median values for each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d15bc-03ce-4993-916c-81908fc3d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc035dc-d9ab-4ae9-b2cf-444599c76aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2d1bf-299f-49e5-a1cd-5a48a4ce048d",
   "metadata": {},
   "source": [
    "You can now impute the missing values by using the `tranform()` method on the `imputer` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e669f-8788-4402-838f-727ae5e907a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db695e-212d-4751-aeff-812c37cd542b",
   "metadata": {},
   "source": [
    "**Todo**: Check the type of the object returned by the `tranform()` method. Is it still a Panda DataFrame?  \n",
    "You can run the following code to wrap `X` back to a Panda DataFrame. Take a little bit of time to understand this instruction, as you are likely to use this kind of wrapping regularly when manipulating data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4c7e0-b7a6-456b-98c7-f86280395414",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a54bee-b473-4778-81d8-fc831a009d80",
   "metadata": {},
   "source": [
    "Run the following two cells to compare the missing values rows before and after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de751e0-e483-43aa-a3ad-f3b68acb6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx = housing.isnull().any(axis=1) # Get the index from the missing value instances in the original data\n",
    "housing.loc[null_rows_idx].head() # 5 first instances with missing values before imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f2f11-798f-4fd3-9544-954ba4d6dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[null_rows_idx].head() # 5 first instances with missing values after imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4921b1-8e0b-4411-9207-3bd8823207cb",
   "metadata": {},
   "source": [
    "Note that you can choose the replace the missing values with other strategies. To do so, you can change the `strategy` parameter passed to the `imputer` instance when you create it, e.g.:\n",
    "- With the mean value of the attribute (`strategy=\"median\"`)\n",
    "- With the most frequent value of the attribute (`strategy=\"most_frequent\"`)\n",
    "- With a constant value (`strategy=\"constant\", fill_value=...`)\n",
    "\n",
    "The two last strategies work also for non-numerical attributes. \n",
    "\n",
    "**Todo**: Modify the code in the previous cells to impute values with a different strategy. If you use a method working with non-numerical attributes, you can include the `ocean_proximity` attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206dda3-b1be-4d07-9644-0ae3497d829b",
   "metadata": {},
   "source": [
    "There exists more advanced imputers in the `sklearn.impute` module. For example, the `KNNImputer` replaces missing values with the mean value of the k-nearest neighbors. The distance between 2 instances is computed using the attributes without missing values for neither instance. Another more advanced imputer is the `IterativeImputer`. It iteratively trains a regression model per attribute and predict the missing values based on all the other attributes.  \n",
    "You do not have to use these different approaches for this tutorial, but do not hesitate to try them out later to see how they perform in comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c3097-462c-43aa-840c-c980a30e0099",
   "metadata": {},
   "source": [
    "**Implementation note**: Scikit-Learn's API is organised around main design principles, using consistent interfaces for the different types of class and objects in the library. For example, the imputers we used previously can be used to run some estimations and transformations on the data. They are a type of *Transformers* object, building on the *Estimators* class. You can learn more about the design rules used in Scikit-Learn API at: https://scikit-learn.org/stable/developers/develop.html#apis-of-scikit-learn-objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90808362-3d2c-4c9f-b36b-eae1c17c8844",
   "metadata": {},
   "source": [
    "### Removing outliers\n",
    "\n",
    "Outliers are data points (instances) that are noticeably different from the rest of the data. There are usually \"out of pattern\" and they can be due to data collection or recording issues. You may want to drop them to prevent your learning algorithm from being influenced by them when training your model.  \n",
    "Detecting outliers is not always an easy problem, and there are several possible approaches and methods you can use. One popular method is the isolation forest algorithm, which is an unsupervised technique using binary trees to detect outliers. Use the `IsolationForest` class from the `sklearn.ensemble` module to perfom outlier detection on your data.  \n",
    "Run the following cell to perform the outlier detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0237f0-8b37-4f65-9c11-fdb7e0cade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5908b-1b0c-4d73-825c-a20a63816566",
   "metadata": {},
   "source": [
    "Notice that we pass `X` as parameter to the IsolationForest `fit_predict()` method. This is because this method takes an array-like parameter, so it would generate an error if we try to pass a Pandas DataFrame.  \n",
    "Run the following two cells to see the output of the detection and the shape of the return value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326914e-c3ea-4c23-81e1-cf7bf9fa9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44747b3-27da-4490-8e36-df59880a2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745500f-29c1-41a0-b68c-15d624c7160e",
   "metadata": {},
   "source": [
    "The returned value is an array containing a number for each instance of the training set. If the number is 1, it means that the instance is predicted as an outlier, if it is -1, it means that the instance is not predicted as an outlier.  \n",
    "You can drop the outliers predicted by the isolation forest, by runing the next cell. We create new variables in case we want to revert to the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfa411-6aed-4ceb-8401-971ed53f9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_no = housing.iloc[outlier_pred == 1]\n",
    "housing_no_labels = housing_labels.iloc[outlier_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e5015-abac-4faf-a83f-a9ad8bbac4e1",
   "metadata": {},
   "source": [
    "If you wish to visualise the outliers, run the following cell to store the outliers and their labels in two new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d9d54-1d77-4f62-9ef3-ff6c81f0ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = housing.iloc[outlier_pred == -1]\n",
    "outliers_labels = housing_labels.iloc[outlier_pred == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60fe90-c246-4109-859b-7f5dc2ccfcaa",
   "metadata": {},
   "source": [
    "Run the next cell to visualise the data after removing the outliers (circle markers) and the outliers on the same scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c5c37-601c-4dad-8a3a-adf96b643287",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=housing_no[\"median_income\"].tolist(), y=housing_no_labels.to_list(), alpha = 0.2, marker=\"o\")\n",
    "plt.scatter(x=outliers[\"median_income\"].tolist(), y=outliers_labels.to_list(), alpha = 0.2 , marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d730a43-6e64-4294-b9b9-8150f7a918bc",
   "metadata": {},
   "source": [
    "Notice that some outliers that were removed do not look so much out of pattern. However, this scatterplot only looks at one attribute (`median_income`) against the target (`median_house_value`). On the other hand, the isolation forest took in consideration all the attributes to predict outliers.  \n",
    "**Todo**: Modify the previous cell to visualise other attributes against the target. Some outliers might make more visual sense if you look at the data from a different \"angle\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7319ee-d2f5-4f10-bcc7-490e00fe6223",
   "metadata": {},
   "source": [
    "## Step 6: Data pre-processing\n",
    "\n",
    "After cleaning your data, you need to perform a few more transformation steps so that it is ready to be used by an algorithm to learn a model. These steps are typically called data pre-processing. \n",
    "\n",
    "In the context of this tutorial, two main pre-processing steps are needed:\n",
    "1. Transforming text/categorical attributes to numerical attributes\n",
    "2. Scaling and transforming the data\n",
    "\n",
    "Depending on the task at hand, you might need to include additional pre-processing steps. For example, if you have a classification task with unbalanced data (i.e., one or more classes are over/under represented in the dataset), you may want to include some preprocessing steps to mitigate this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b7fb1-2fb9-4a8d-98fd-ae03460d4d1b",
   "metadata": {},
   "source": [
    "### Transforming text/categorical attributes to numerical attributes\n",
    "Remember that the attribute `ocean_proximity` is a categorical attribute with text values. You can see this by looking at a few of its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8c853-4576-4726-a820-eafd22a5c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a65424-ea31-42eb-8f37-39746dafcbf3",
   "metadata": {},
   "source": [
    "Categorical means that it can take a limited number of text values, each corresponding to a category.  \n",
    "Since most machine learning algorithms work only with numerical values for the attributes, you usually need to convert text categories to numbers.  \n",
    "The most straighforward way to do this is to convert each different text category to a different number. This called *ordinal encoding*. You can use the Scikit Learn's `OrdinalEncoder` to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940f689-192b-4a20-8575-fb028cee65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cf5dd-8292-4b48-b0ef-3aedc0835d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b18e-6804-4e8a-bc67-cb96e92b08f6",
   "metadata": {},
   "source": [
    "You can access the different categories using the `categories_` field of the `OrdinalEncoder` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba88d73-996a-4f85-9175-f5229a080218",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80806b7-72e7-4117-af8e-905856fe92c3",
   "metadata": {},
   "source": [
    "The category \"<1H OCEAN\" is converted to 0, \" INLAND\" to 1, \"ISLAND\" to 2, \"NEAR BAY\" to 3 and \"NEAR OCEAN\" to 4.\n",
    "\n",
    "The main issue with this type of representation is that a machine learning algorithm will assume that close values are corresponding to more similar categories than distant values. It might be fine in some cases (e.g., if your category has ordered values such as \"bad\", \"average\", \"good\", \"excellent\"), but it does not make sense in our case. Indeed, categories 0 and 4 are for example conceptually closer than categories 0 and 1. This will be problematic because a machine learning algorithm would learn the opposite.  \n",
    "\n",
    "A common fix is to encode the categories with a group of binary attributes. Each binary attribute correspond to a category and is set to 1 if the instance is part of this category, 0 otherwise. This is called *one-hot encoding*, because for each instance only one attribute is set to 1 (hot), while the others are set to 0. The new attributes are sometimes called *dummy* attributes.  \n",
    "You can use the Scikit Learn's `OnHotEncoder` class to convert categorical values into one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6ae83-7967-42bf-9560-8075faddcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1613759-28fa-41df-8c4a-0cf59b3f9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192701c-5cae-4078-8ea5-ff935320266d",
   "metadata": {},
   "source": [
    "Notice that by default, the `transform` and `fit_transform` method returns a Scipy *sparse matrix* instead of a NumPy array.\n",
    "A sparse matrix is a very efficient representation of a matrix containing a lot of zeros. This is the case for the output of one-hot encoding with a large number of categories. In this case, using this data structure will speed up computations and save memory. You can use a sparse matrix much like a 2D-array (see SciPy documentation), but if you prefer to work with NumPy arrays, you can convert it with the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ca9f8-d99e-4fff-8b78-e574f205f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbe615-66bf-46e5-b5c6-4f3c7cba3416",
   "metadata": {},
   "source": [
    "Alternatively, you can also set `sparse_output=False` when creating the `OneHotEncoder` instance for the `transform` and `fit_transform` methods to directly return a regular NumPy array:\n",
    "\n",
    "```Python\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8e5f5-48e5-4fcd-97ea-85830a4607e6",
   "metadata": {},
   "source": [
    "Pandas also offers a function called `get_dummies()` to convert categorical attributes to one-hot encodings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522d1b5-a954-41a8-934e-cfe8aa55d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\n",
    "pd.get_dummies(df_test, dtype= int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75531c74-028b-4ffd-9299-bca795f3a647",
   "metadata": {},
   "source": [
    "However, the advantage of using the Scikit-Learn's `OneHotEncoder` class is that once a `OneHotEncoder` instance is fitted to produce an encoding for some training data (with the `fit` method), it will always output the same number of *dummy* categories when used to transform (with the `transform` method) new data. This is important as we want the transformation process to be consistent between the model training phase and when it is used to make new predictions. \n",
    "For example, when transforming the same `df_test` than in the previous cell, look at what our `cat_encoder` outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4843ee1-6b51-48d4-b5ef-2007abfc4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.transform(df_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c3cef-0270-4d38-ac7b-2d2f3bc9a3f9",
   "metadata": {},
   "source": [
    "It outputs encodings with values for the 5 categories, where the Pandas `get_dummie()` outputs encodings with only 2 values for the 2 categories in `df_test`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddaa5f-2d96-46d2-abc7-92225467b604",
   "metadata": {},
   "source": [
    "Another advantage of Scikit-Learn's `OneHotEncoder` class is the way it handles unknown categories. If you pass data with a new category which is was not in the data used to fit the encoder, a `OneHotEncoder` instance will raise an error, where the Pandas `get_dummies()` function will output an encoding with a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53b5a3-b6b4-468e-a4d9-45556c9f90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
    "pd.get_dummies(df_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb601b-773d-428a-a8fe-e8e3a1b0b13f",
   "metadata": {},
   "source": [
    "If you run:\n",
    "```Python\n",
    "cat_encoder.transform(df_test_unknown).toarray()\n",
    "```\n",
    "you will get a ValueError stating:\n",
    "`ValueError: Found unknown categories ['<2H OCEAN'] in column 0 during transform`\n",
    "\n",
    "If you want your `OneHotEncoder` instance to handle any unknown category, you can set the `handle_unknown` field to `\"ignore\"`. In this case, it will represent any unknown category with only zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234222f9-4813-4028-a9d5-8b98f9ee8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.handle_unknown = \"ignore\"\n",
    "cat_encoder.transform(df_test_unknown).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cab63-e9f0-4ca2-a5e6-5b3024aaea3a",
   "metadata": {},
   "source": [
    "Note that if a categorical attribute has a lot of categories, using one-hot encoding will results in a very large number of new binary, *dummy*, attributes. This might end up slowing down training and degrade performances. If that is the case, one option can be to design a numerical attribute to replace the categorical attribute, containing the same information. For example, the `ocean_proximity` could be replaced by a numerical attribute containing the distance to the ocean. Alternatively, you can use one of the encoders provided by the Category Encoder package (https://contrib.scikit-learn.org/category_encoders/).  \n",
    "\n",
    "In the case of neural networks, which we will focus on in the following tutorials, categorical attributes can be replaced with low-dimensional vectors, learned through the training phase. These vectors are called *embeddings*. These embeddings are particularly used in Natural Language Processing as the data is mostly textual. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493b959-5d4b-437f-8f9a-5dbeee530d94",
   "metadata": {},
   "source": [
    "### Scaling and transformations\n",
    "\n",
    "Scaling is one of the most important pre-processing steps, as most machine learning algorithms do not perform well if the numerical attributes have very different scales.  \n",
    "This is the case for the housing dataset we are using here. For example, the total number of rooms ranges from about 6 to 39 320, while the median income only ranges from 0 to 15. Without scaling in the pre-processing phase, most machine learning algorithms will ignore the median income when learning the model, and focus more on the total number of room. This is because it would consider that the variables in the median income are too small compared to the ones in the total number of rooms, and therefore not useful to describe the patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e607d4d-f357-4ddf-8e0d-154f9b4e1691",
   "metadata": {},
   "source": [
    "As with all pre-processing transformations, scaler should be trained/fitted on the training data only. Once trained, they can be used to transform the validation set, test set and new data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5fc19-a907-435d-91f9-36416fdadaa5",
   "metadata": {},
   "source": [
    "There are two main approaches to get the attributes to have to same scale:\n",
    "- Min-max scaling (also called normalisation)\n",
    "- Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f28c0a-0878-444f-bf43-88b0752ed112",
   "metadata": {},
   "source": [
    "#### Min-max scaling\n",
    "Min-max scaling is the simplest approach. For each attribute, the values are shifted and rescaled so that they fit in a range from 0 to 1 (or another specified range). \n",
    "This is achieved by applying the formula:  \n",
    "$\\Large x_{scaled}=\\frac{x - x_{min}}{x_{max}-x_{min}}$  \n",
    "where $x$ is a value for a specific attribute, $x_{scaled}$ the scaled value, $x_{min}$ and $x_{max}$ the min and max values of this attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cb0f6-2212-4204-9bf0-760ab2b48506",
   "metadata": {},
   "source": [
    "The Scikit-Learn's `preprocessing` module contains several standard scalers, among which the `MinMaxScaler`.\n",
    "Run the next cell to perform min-max scaling on the numerical attributes of the housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e3c3b-4693-4490-b602-ffc10fa32136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948dcb4-7517-4db7-be6d-9c94e630d71f",
   "metadata": {},
   "source": [
    "The `fit_transform` returns a NumPy array. You can run the next cell to convert it back to a Pandas DataFrame, and plot the attributes to see the effect of the min_max_normalisation (look at the x axis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375b31d-7083-44d0-b0f8-61e479b8adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_min_max_scaled = pd.DataFrame(housing_num_min_max_scaled,\n",
    "                         columns=min_max_scaler.get_feature_names_out(),\n",
    "                         index=housing_num.index)\n",
    "df_housing_num_min_max_scaled.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f9a78-1841-40bb-b3e5-4eb00d034db0",
   "metadata": {},
   "source": [
    "#### Standardisation\n",
    "One of the disadvantages of min-max scaling is that it is sensitive to outliers. If you have an abnormally high value in your data, out of the normal range of values, it will map it to 1, and \"squish\" all the other values in a very small range. \n",
    "\n",
    "Standardisation on the other hand is not much affected by this. It works by substracting the mean value of the attribute (so the standardised data will have a mean equal to 0) and it divides the result by the standard deviation of the attribute (so the standardised data will have a standard deviation equal to 1):  \n",
    "$\\Large x_{standard}=\\frac{x - \\mu}{\\sigma}$  \n",
    "where $x$ is a value for a specific attribute, $x_{standard}$ the standardised value, $\\mu$ the mean of this attribute and $\\sigma$ its standard deviation.\n",
    "\n",
    "You can use the Scikit-Learn's `StandardScaler` transformer to perform standardisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d392b-642c-485f-a2e9-dda74d15cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d9796-dc35-4b14-a01b-a32009221e07",
   "metadata": {},
   "source": [
    "You can run the next cell to convert the scaled data back to a Pandas DataFrame, and plot the attributes to see the effect of the standardisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fb87b-2720-4b64-ac6d-156d1f32082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_std_scaled = pd.DataFrame(housing_num_std_scaled,\n",
    "                         columns=std_scaler.get_feature_names_out(),\n",
    "                         index=housing_num.index)\n",
    "df_housing_num_std_scaled.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0e145-0d8f-4398-88df-76b08b23f429",
   "metadata": {},
   "source": [
    "Note that if you want to apply scaling to a sparse matrix, you can either tranform it to a dense matrix first and run the StandardScaler normally, or use a StandarScaler(with_mean = False). This will only divide the data by the standard deviation, and not substract the mean, because it would break sparsity for a sparce matrix, as a lot of elements would not be equal to 0 anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce083bc0-414b-4bca-80da-af1c7c5a6217",
   "metadata": {},
   "source": [
    "#### Data distribution transformation\n",
    "\n",
    "Most machine learning algorithms works best with data following a Gaussian distribution, also known as Normal distribution. Therefore another important step is to transform attributes so that their distribution is close to a Gaussian distribution.  \n",
    "When the attribute distribution has a *heavy tail*, i.e., data far from the mean are not exponentially rare, a common transformation is to replace the attribute by its square root or to raise it to a power beween 0 and 1. If the tail is very long, such as if it follows a power law distribution, then you can replace the attribute with its logarithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf8436-44a4-4268-8d07-f20277102d16",
   "metadata": {},
   "source": [
    "**Todo**: Several features have a heavy tail in the housing dataset. Identify them, and, for one of them, fill the following code to try the squared root and logarithm transformations and see which one brings it closer to a Gaussian distribution (i.e., bell-shaped). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8f47b-2674-4e11-a492-3ba8beaa09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3), sharey=True)\n",
    "# replace attribute_name by the chosen attribute name in the 3 next lines\n",
    "housing[\"attribute_name\"].hist(ax=axs[0], bins=50) \n",
    "housing[\"attribute_name\"].apply(np.sqrt).hist(ax=axs[1], bins=50) \n",
    "housing[\"attribute_name\"].apply(np.log).hist(ax=axs[1], bins=50) \n",
    "axs[0].set_xlabel(\"Chosen attribute\")\n",
    "axs[1].set_xlabel(\"Sqrt of attribute\")\n",
    "axs[2].set_xlabel(\"Log of attribute\")\n",
    "axs[0].set_ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7895a4-594f-46fa-a93c-99ac01782335",
   "metadata": {},
   "source": [
    "Note that you should perform this kind of distribution tranformation before normalising or standardising the data. \n",
    "\n",
    "There are other ways to deal with heavy-tailed distributions, such as using *bucketisation*. This consists in chopping the distribution into roughly equal-sized busckets, and replacing each attribute value by the index of the buckets it belongs to (much like what we did with the `income_cat` attribute earlier). You can find an example of bucketisation in code cell 82 of Aurélien Géron's __[end_to_end ml project notebook](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb)__)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df1454-80ce-4287-b03f-cd79958f44bd",
   "metadata": {},
   "source": [
    "Another common type of distribution you can find in data is the multimodal distribution. This distribution is recognisable by several peaks, or *modes*, in the data. This is the case for example for the housing median age, the latitude and the longitude attributes in the housing dataset. You could also use bucketisation to transform this type of distribution, but this time by treating the index of the bucket as categories, rather than using numerical values. Because we are then dealing with a categorical attribute, you will need to encode it with one-hot encoding for example.  \n",
    "Another approach to transform multimodal distributions is to add an attribute for each of the modes observed in the data. Each new \"mode attribute\" contains the similarity between the multimodal attribute value and that particular mode. Similarity is typically computed using a *radial basis function* (RBF), such as the Gaussian RBF. This function outputs a value decaying exponentially as the input values move away from a fixed point (i.e., as the radius increases). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48013754-7eb5-48b7-9306-565d2d307676",
   "metadata": {},
   "source": [
    "You can implement such transformations (sqrt, log, rbf, etc.) using the Scikit-Learn's `FunctionTransformer` transformer. Run the two next cells to:\n",
    "- performing a log transformation on the population attribute,\n",
    "- create an attribute based on the similarity with the mode at age 35 of the housing median age, using an RBF funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea02e3-d415-429f-895a-fb6b9967d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3ff47-ec62-4729-91ec-af107625c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fa8ed-4a86-4a8e-9672-bdebc81f93f8",
   "metadata": {},
   "source": [
    "`FunctionTransformer` is handy and easy to use, but it does not allow to train your transformer. For example, what if you want to create a transformer which learns different geographical clusters using the longitute and latitude attributes of the training data, and output a number of new attribute representing how close each data instance is from each identified cluster?  \n",
    "You will need to define your own custom transformer.  \n",
    "**Todo**: Study the following code. It defines a custom tranformer that uses the `KMeans` clusterer to identify geographical clusters in the `fit()` method. K-Means is a unsupervised clustering algorithm that can be used to find clusters in data. Here, it is configured to look for 10 clusters. The similarity of each instance to each cluster is computed in the `transform()` method using a rbf kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9d1f4-a173-441f-92d5-ae02fb8820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9e595-b1c8-4546-a111-fa8f195de3ea",
   "metadata": {},
   "source": [
    "If you are not familiar with defining classes in Python, you can have a look at the following tutorial: https://www.w3schools.com/python/python_classes.asp.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bcac8-f328-467c-8066-3ccb843855aa",
   "metadata": {},
   "source": [
    "You can now use this custom transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328317a-bf2a-4b1e-be10-4d5d2798d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
    "                                           sample_weight=housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a018e7-12c2-4b90-b53e-020a87a5748c",
   "metadata": {},
   "source": [
    "Run the following cell to visualise the 10 identified cluster centers as well as the level of geographic similarity of each disctrict to its closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4695d7-8bcc-4eab-8b2b-194bb97b27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
    "\n",
    "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
    "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(10, 7))\n",
    "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
    "         label=\"Cluster centers\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68673ed-8d5d-4bbf-a626-70241cb9e12a",
   "metadata": {},
   "source": [
    "We have seen a number of pre-processing steps to transform the data in a form more adapted to train a model.  \n",
    "We will now see how we can bring all of these together in a transformation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e0c5c-a58d-46fc-9f9c-b8154819ef74",
   "metadata": {},
   "source": [
    "### Step 7: Transformation pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca87368-eb0a-4e20-8224-761c6fc15b41",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a `Pipeline` class to build a sequence of transformations. You can see in the next cell a simple pipeline for numerical attributes, which first imputes missing values and then scales the input attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b99ba-7149-4f69-be40-2e994876ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50104e-f9f6-46ae-bffc-6e6783ab1f61",
   "metadata": {},
   "source": [
    "You can visualise the pipeline by running a cell with `num_pipeline` as the last line. The visualisation is interactive and allows you to expand each step to see what parameters are used (e.g., `SimpleImputer` uses the median to impute missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3189e-4beb-4ee2-a65e-2b44b7ff861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0736f-6066-428c-adba-8797ad323fea",
   "metadata": {},
   "source": [
    "You can also use the `make_pipeline()` function for a lighter syntax, and if you do not wish to name each transformer in your pipeline (standard names will be given to each of them, using the corresponding transformer's class name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b29e0-05b9-4ff5-a512-4a5725a6ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc060b-6177-40d3-af2d-f3908ca9cab3",
   "metadata": {},
   "source": [
    "You can also set the visualisation of the pipeline to a simple diagram if you do not want to use the interactive visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf5367-2ac6-410e-952e-e7220c0bef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='text')\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989f0f8-e635-4161-b937-ab282328744d",
   "metadata": {},
   "source": [
    "Run the following cell to apply this pipeline to the numerical attributes of the housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234f92c-9970-4648-913c-45e05c6a8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19dead-146a-48bf-b848-87a874d77837",
   "metadata": {},
   "source": [
    "You can run the following cell if you want to visualise the resulting attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08164887-0a1c-4dd5-908d-9d7daf8ae0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(housing_num_prepared,\n",
    "                         columns=num_pipeline.get_feature_names_out(),\n",
    "                         index=housing_num.index)\n",
    "df_housing_num_prepared.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7c816-a5dc-47ba-90ea-df3c29abdcb5",
   "metadata": {},
   "source": [
    "We have handled numerical and categorical attributes seperatly so far. However, it is possible to define a single tranformer able to deal with both apply the appropriate tranformations to each attribute given its type. For this, you can use the `ColumnTransformer`.  \n",
    "For example, we define in the next cell a `ColumnTransformer` applying different transformations to attributes listed as numerical and attributes listed as categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee9a75-e8b7-4543-912c-b8fcd958fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da52a9-9f66-4793-844f-b0741d72b6ad",
   "metadata": {},
   "source": [
    "It might not be practical to list all the attributes names, especially if you have a lot of them initially. Thankfully, you can use the `make_column_selector` function to automatically select the attributes of a specified type. Similarly as `make_pipeline` previously, you can also use the `make_column_transformer` function to avoid naming the transformers yourself when defining a `ColumnTransformer`. The code in the next cell creates a `ColumnTransformer` by applying our `num_pipeline` to selected attributes with data type `np.number`, and our `cat_pipeline` to selected attributes with data type `object` (which is used for text in Pandas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f539e-9d50-44be-906a-0cd798f83ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b3f7e-d3b3-44bf-b1fa-0c645a0d2fc3",
   "metadata": {},
   "source": [
    "Run the following cell to apply this pipeline to all the attributes of the housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696212ec-1701-49cd-b03d-c8a4b7d431d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cae5e3-7fa3-4caf-9157-20d7aa31db93",
   "metadata": {},
   "source": [
    "You can run the following cell if you want to visualise the resulting attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a01a2-de2c-4fc9-a38f-e70071877eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_prepared = pd.DataFrame(housing_prepared,\n",
    "                         columns=preprocessing.get_feature_names_out(),\n",
    "                         index=housing.index)\n",
    "df_housing_prepared.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd540b1-f13f-46dd-be08-5e5eedee6d7e",
   "metadata": {},
   "source": [
    "Notice that `num_pipeline` and `cat_pipeline` are automatically named respectfully `pipeline_1` and `pipeline_2` and that the categorical attribute `NEAR_OCEAN` is replaced by five numerical binary attributes because of the one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2f2d8-fe36-4295-b1d6-5e56ea6e49ab",
   "metadata": {},
   "source": [
    "#### Creating our final preprocessing pipeline\n",
    "Here is a list of steps we want our final preprocessing pipeline to perform on the housing dataset:\n",
    "1. Impute missing values for the numerical attributes using their median, and using the most frequent category for categorical attributes.\n",
    "2. Use one-hot encoding to transform categorical attributes to numerical attributes.\n",
    "3. Compute and add a few ratio attributes: `bedrooms_ratio`, `rooms_per_house` and `people_per_house`.\n",
    "4. Add cluster similarity attributes using the `ClusterSimilarity` transfomer we defined.\n",
    "5. Replace heavy_tailed attributes with their logarithm.\n",
    "6. Standardise all numerical attributes.\n",
    "\n",
    "**Todo**: Discuss the reason why we apply each of these steps. Then, study the code below and match each of the steps listed above with the code corresponding to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d3a1f-70f5-4eb9-8545-a68972180add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5176c6b-2502-427d-ab59-ec242fcef2d3",
   "metadata": {},
   "source": [
    "**Todo**: Finally, complete the code below to apply this pipeline to the `housing` DataFrame. Print the shape of the output and visualise the resulting attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f8843-7265-4d95-a5f3-5108a8943edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = # Complete the code here\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717e65e-a8aa-4d5f-b440-4668a5fb2a4b",
   "metadata": {},
   "source": [
    "Run the following cell to get the name of the resulting attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef5044-ef26-48b1-8666-716c5169c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399190c-6179-40b5-9b21-163ab3f5b111",
   "metadata": {},
   "source": [
    "**Todo**: Insert a cell below and write some code to visualise the resulting attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445787b-c171-42f9-81b3-68ccf3cac8db",
   "metadata": {},
   "source": [
    "## Step 8: Other ways of getting and loading data\n",
    "\n",
    "At the start of this tutorial, we loaded data from a GitHub repository using the URL of the repository, but there exists other ways to get and load date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953aace3-9506-44c6-862a-da5617c4fa31",
   "metadata": {},
   "source": [
    "### Loading data from a package\n",
    "You can also load some datasets directly from Python packages. For example, Scikit_Learn provides a few toy dataset, such as the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aaf436-117f-407a-aeb2-1e42c506412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df_iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2f80d-0370-4d38-bb05-03ee5ebe2668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a0431-d1a8-4a5a-ac2e-3885b1e34568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fb183-03a6-4da4-b470-21a4122ad9a5",
   "metadata": {},
   "source": [
    "### Framework specific loaders\n",
    "Some libraries like TensorFlow and PyTorch have there own data types, classes and methods to load and manipulate datasets.  \n",
    "Note that the next cell will take a few seconds to run. It not only loads the MNIST dataset (images) from a distant repository, but it also prepares the dataset by splitting it into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679c57f-272e-4e97-ba1f-7cf98adb83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c3fb4-875a-4a1b-90e7-6d1041c5393d",
   "metadata": {},
   "source": [
    "The code in the following cell does a similar job using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a00c0d-d8fb-4098-910c-7629e68633d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "ds_train = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "ds_test = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe1c77-d61a-4c5b-bdec-8fe8083c4237",
   "metadata": {},
   "source": [
    "### Download data \"manually\"\n",
    "You can also simply download the dataset (or build it yourself and store it with the appropriate format in a csv file) on your local machine and load it into a Pandas DataFrame with the `pandas.read_csv()` function.  \n",
    "However, this might not be possible to download and store very large datasets. These types of datasets are usually stored and accessible on distant repositories, and you can download them directly using the URL of the repository and a custom helper function like the one we designed at the start of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4fcb5-8154-43b8-9a34-a70f82ce4d2a",
   "metadata": {},
   "source": [
    "### Web scrapping\n",
    "Lastly, one way to get data in the era of the internet is to scrap it from webpages. We will not cover web scrapping in this course, but this can be an intesresting skill for you to develop. You can find a good tutorial about it at the following link: https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a81a1a-b743-4533-a126-324c84b17d53",
   "metadata": {},
   "source": [
    "## Step $\\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff599a-a132-435d-aae4-10123bfd2d69",
   "metadata": {},
   "source": [
    "You can now load another dataset, explore it, and create an adapted pre-processing pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:COMPSCI714]",
   "language": "python",
   "name": "conda-env-COMPSCI714-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
